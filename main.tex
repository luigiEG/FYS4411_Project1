\documentclass{article}
\usepackage{algorithm}
\usepackage{listings}

\usepackage{algpseudocode}
\usepackage{graphicx} % Required for inserting images

\title{CP_Project1}
\author{Luigi Ernesto Ghezzer}
\date{April 2024}

\begin{document}

\maketitle

\section{Introduction}
We will study a BEC system, both for non interacting bosons, and with an hard sphere interacting bosons with VMC. The goal is to obtain an upper limit of the ground-state.\\

In a first part we will describe the algorithms we will implement. 

\section{Algorithms}
Firs we present two samplers for obtaining the mean energy for a given variational paramenter, Metropolis and Fokker-Plank. Then the calibration process, a simple method to automatically obtain rasonable sampler hyperparameters,  ${\tt dt}$ and ${\tt subgroup\_size}$ ,that lead to optimal acceptance rate. In a third part we present the optimization process for obtaining optimal variational parameters. Then we present the method used for evaluating the uncertanties on the mean energies, based on the blocking alghorithm. In the last part we present the implementation of the numerical derivation for obtaining the laplacian. 
\subsection{Metropolis–Hastings}
Metropolis–Hastings is one of the most simple algorithm for obtaining markov chain from a probability distribution (also un normalized, since need just the ratio in the acceptance function). His semplicity lays in the symmetrical proposing distribution that gives a simple solution for the acceptance function such that in a steady state the alghoritm will sample a certian probability distribution. In our implementation the proposing distribution is a normal distribution with mean 0 and standard deviation \texttt{dt} and the proposed number of particles moved for step is \texttt{subgroup\_size}
\begin{lstlisting}
def propose:
    subgroup_indexes = random choice subgroup_size 
                       indexes between (0,N-1)
    dX = dt*NormalDist(0,1)[subgroup_indexes]

def test:
    Uniform(0,1) < P(X_new)/P(X_old)
\end{lstlisting}
Under general condition, depending on the dimension of the problem, the optimal acceptance rate is between (0.23-0.5) \cite{MetroAcc}. In the actual implementation of the code the likehood term in the test part is evaluated using the wavefunction in log domain to better andle small quantities and reduce the exponentiation.

\subsection{Fokker-Plank}
Fokker-Plank walker or Metropolis-adjusted Langevin algorithm (MALA) is another alghorithm for obtaining markov chain from a probability ditribution. As the Metropolis-Hasting can sample also from an unnormalized function. The intuitive idea around MALA is that the walker is guided towords higer probability region of phase space and this lead to faster convergence in some cases\cite{MALABest}. The main ingridient are the overdumped Langevin SDE, diffusion equation and the detailed balance:
\begin{equation}
        \dot X = v + \eta_{\mathbf{dt}} 
\end{equation}
\begin{equation}
        \eta \sim N(0,\mathbf{dt})
\end{equation}
\begin{equation}
    \frac{\partial P}{\partial t} = -\frac{\partial}{\partial x} (vP) + \frac{\partial^2 P}{\partial x^2}
\end{equation}
Asking for:
\begin{equation}
    \frac{\partial P}{\partial t} = 0
\end{equation}
The solution is given by:
\begin{equation}
    v = 2\frac{\nabla \sqrt{P}}{\sqrt{P}}
\end{equation}
So one need the gradient of the wavefunction, this can be obtained analytically for simple cases or numerically. Since the proposing distribution is now biased one have to adjust in the acceptance ratio function. This can be done just recognizing that the proposed probability is given by:
\begin{equation}
    G(y, t+dt; x, t) = \frac{1}{Z} exp\Big[-\big(y-(x+v dt))\Big]
\end{equation}
The rest of the algorithm is the same as the Metropolis-Hastling. Again this alghoritm have two hyperparameter \texttt{dt} and  \texttt{subgroup\_size}. In general the optimal acceptance rate is 0.57 \cite{MALAAcc}.

\subsection{Calibration}
The two sampler we are usign have two hyperparameters:
\begin{itemize}
    \item ${\tt dt}$, the norm of the random displacement in phase space during the proposing step of the sampler
    \item ${\tt subgroup\_size}$ the number of particle moved each proposing step
\end{itemize}
In general, one is interested with a fixed number of steps, in obtaining the chain that lead to the lowest possible error on the mean energy (a proper evaluated error, like with the blocking alghoritm). The relation between the final error and the statistical properties of the chain is complex\cite{Pederiva}. One of the most "direct" measure of the quality of a chain is her autocorrelation exponent, the less a chain is autocorrelated the most real independent sample it contains. Even if the relation is complex there are some rule of thumb in literature. In particular for a specific sampler there are an acceptance rate that tends to minimize the final error on the mean energy.\\ 

Since we are not expert on the sampler hyperparameter, we decided to implement a calibration method, that can use part of the warmup steps to find hyperparameters that lead to a specific acceptance rate. The intuitive idea is to reduce/augment the displacement in phase space if the acceptance rate is smaller/bigger than the acceptance rate goal. One can reduce the phase space displacement in two ways, reducing the \texttt{subgoup\_size} or \texttt{dt}. We decided to randomly choose each time between the two possibilities.

\begin{lstlisting}
def calibration(calibraten_steps, batch_steps, 
                acceptance_rate_goal, factor):
    for batch in range (calibraten_steps//batch_steps)
        batch_chain = Walker.get_chain(steps=batch_steps)
        if batch_acceptance_rate > acceptance_rate_goal:
            choose randomly between (dt, subgroup_size) and
            multiply by 1/factor
        if batch_acceptance_rate < acceptance_rate_goal:
            choose randomly between (dt, subgroup_size) and 
            moltiply by factor
\end{lstlisting}

At the end just tuning the \texttt{factor} paramenter and using half of the warmup steps for the calibration we will be able to automatically initialize the hyperparamenters of the walker. The part of the chain used in the calibration is discarded as the warmup part, but is still useful to reach the steady state.


\subsection{Uncertanties}
Steps in a markov chain are not independent. So we cant estimate the error on our mean energy just calculating the standard deviation and divide by the square root of number of samples. Different approches exhist for evaluating the error on the mean of correlated series. Some are based on evaluating autocorrelation coefficient and discarding samples of the chain in such a way that only independent samples are left. The disadvantages of this kind of methods is that the autocorrelation needs nested for cycle to be calculated. We used insted the blocking method \cite{Block}. The block transfrom of a random variable in a time series is given by:
\begin{equation}
    x^\prime_i = \frac{1}{2}(x_{2i}+x_{2i+1})
\end{equation}
It is demostrated that the mean and the standard deviation of the mean are fixed point of this transformation. Studying the series $(\sigma({x_i}), \sigma({x^\prime_i}),\sigma({x^{\prime \prime}_i}), ...)$ as a function of the number of transformation and if platue is reached, one have a estimate from above of the uncertanties on the mean.\\ 

In our implementation of the blocking alghoritm the uncertanty on the mean is taken as the 0.8 quantiles from above of the $\sigma$ series to avoid taking the lastly largely fluctuating point. If the platue is reached is judged by eye. For this reason at the end of a energy evaluation of the VMC a plot with the series is saved.

\subsection{Optimization}
To obtain the parameter that minimize the variational mean energy we implemented a gradient descent method with a decaying learning rate schedule. One can demostrate that:
\begin{equation}
    \frac{\partial E}{\partial \alpha} = 2\Big(\langle \frac{1}{\psi}\frac{\partial \psi}{\partial \alpha} E_L \rangle - \langle  \frac{1}{\psi}\frac{\partial \psi}{\partial \alpha} \rangle \langle E_L \rangle \Big)
\end{equation}
So the gradient of the mean energy in the variational parameter space can be obtained from a chain as the mean energy. Our implementation of the gradient descent is:
\begin{lstlisting}
def optimize(optimization_steps, decay):
    for step in optimization_steps:
        grad = get_gradient(params)
        params = params - eta*grad
        eta = -decay*eta
\end{lstlisting}
Where \texttt{decay} is an hyperparameter.

\subsection{Numerical derivation}
We will implement also a method where the laplacian needed in the local eneryg calculation is obtained numerically insted of analytically. This is in general useful when analytical result are not disponible. Since numerical deriavation need to make differences between small quantities, it is useful to work in the log domain.
\begin{equation}
    E_{Kin} = -0.5\frac{\nabla^2 \psi(x)}{\psi} = -0.5\frac{\nabla^2 e^{f(x)}}{e^f} = -0.5\Big(\nabla^2f - (\nabla f)^2\Big)
\end{equation}
Then to obtain the derivative and the laplacian we used simply finite difference approximations on $f$, the wavefunction in log domain. Another advantage of working in log domain is to avoid to exponent.
\section{Non Interacting}
In this section we will study the case of non interacting boson trapped in an spherical harmonic oscillator. In a first part we present some analytical results useful for implementing the sampler and the variational mean energy. In a second part we test the variuos algorithm in obtaining the ground state energy since we have an analytical result to compare. In a third part we present a grid search, optimization example and a comparison between the variuos algorithm.\\

The hamiltonian take the form:
\begin{equation}
    H = \sum_i^N -\frac{\nabla^2_i}{2} + V_{ext}(\mathbf{r}_i)
\end{equation}

\begin{equation}
     V_{ext}(\mathbf{r}_i) = \frac{1}{2}\omega_{ho}r_i^2
\end{equation}
Since the hamiltonian is separable, the autostate has to be separable too. This means that we can take as a variational wavefunction:
\begin{equation}
    \psi_T(r_1, r_2, ...; \alpha) = exp\big[-\alpha(r_1 + r_2 + ...) ]
\end{equation}
With just one variational parameter $\alpha$.Note that this is the right ground-state for $\alpha = 0.5$, infact this is just N independent harmonic oscillator. From now on we are going to take as reduced energy unit $\omega_{ho}$. 
We can work out analytically the local energy applying the laplacian:
\begin{equation}
    E_L = N D \alpha + (0.5 - 2\alpha^2)\sum_i^N r_i^2
\end{equation}
Where D is the dimension of the system. The analytical ground state energy is given by:
\begin{equation}
    E_0 = 0.5  N  D
\end{equation}
The quantum force will be, again analytically:
\begin{equation}
    v = \frac{2\nabla \mathbf{\psi_T}}{\mathbf{\psi_T}} = 4 \alpha (\mathbf{r_1}, \mathbf{r_2}, ..)
\end{equation}

\subsection{Ground state}
In the non interactive case we know that with $\alpha = 0.5$ then $\psi_T$ is the gorund state of this hamiltonian. This mean that if the local energy is calculated analitically:
\begin{equation}
    E_L(x) = \frac{\langle x | H | \psi_T \rangle}{\langle x | \psi_T \rangle} = \frac{E_0\langle x | \psi_T \rangle}{\rangle x | \psi_T \langle} = E_0
\end{equation}
in \texttt{src/NonInteracting/SHOscillatorMetropolisAnalytical\_GroundState} and \texttt{src/NonInteracting/SHOscillatorFokkerPlankAnalytical\_GroundState} one can find the code for evaluating the ground state energy but is clear from the equation that the result will be the analytical one $0.5ND$ with zero variance (another suggestion that $\psi_T$ is an autostate of the hamiltonian). \\

A little more of interest is the case were the hamiltonian is calculated numerically. In this case numerical error induced by the finite difference approximation lead to a finite error on the mean energy. We present the result from \texttt{src/NonInteracting/SHOscillatorMetropolisNumerical\_GroundState} obtained with a Metropolis sampler but the local energy is calculated with numerical differentiation.\\



\begin{thebibliography}{9}
\bibitem{MetroAcc}
Weak convergence and optimal scaling of random walk Metropolis algorithms
A. Gelman, W. R. Gilks, G. O. Roberts
Ann. Appl. Probab. 7(1): 110-120 (February 1997). DOI: 10.1214/aoap/1034625254

\bibitem{MALABest}
Exponential Convergence of Langevin Distributions and Their Discrete Approximations
Gareth O. Roberts and Richard L. Tweedie, Bernoulli
Vol. 2, No. 4 (Dec., 1996), pp. 341-363 (23 pages)

\bibitem{MALAAcc}
Optimal Scaling of Discrete Approximations to Langevin Diffusions 
Gareth O. Roberts, Jeffrey S. Rosenthal
Journal of the Royal Statistical Society Series B: Statistical Methodology, Volume 60, Issue 1, January 1998, Pages 255–268, https://doi.org/10.1111/1467-9868.00123

\bibitem{Pederiva}
Variational and diffusion monte carlo approaches to the nuclear few- and many-body problem
Francesco Pederiva, Alessandro Roggero, Kevin Schmidt, Lect.Notes Phys. 936 (2017) 401-476

\bibitem{Block}
Error estimates on averages of correlated data 
H. Flyvbjerg; H. G. Petersen
Crossmark: Check for Updates
J. Chem. Phys. 91, 461–466 (1989)
https://doi.org/10.1063/1.457480


\end{thebibliography}

\end{document}


